---
title: "Loading NCEP/NCAR Reanalysis"
author: "Enrique Valero Leal"
date: "03/02/2022"
output: html_document
---

```{r setup, include=FALSE}
if(!require("knitr")) install.packages("knitr")
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

## Basic dependencies
```{r install dbnR, include=FALSE}
# Basic usage
if(!require("Rcpp")) install.packages("Rcpp", dependencies = TRUE)
library(Rcpp)
if(!require("devtools")) install.packages("devtools")
library(devtools)
```

## Installation of packages necessary for bnlearn/dbnR

```{r install dbnR, include=FALSE}
# Basic usage
if(!require("bnlearn")) install.packages("bnlearn")
if(!require("data.table")) install.packages("data.table")
if(!require("dbnR")) devtools::install_github("dkesada/dbnR")
# Network visualization
if(!require("visNetwork")) install.packages("visNetwork")
if(!require("magrittr")) install.packages("magrittr")
if(!require("grDevices")) install.packages("grDevices")
```

## Installation of packages necessary for NCEP/NCAR

```{r install RNCEP, include=FALSE}
if(!require("RNCEP")) install.packages("RNCEP")
if(!require("lubridate")) install.packages("lubridate")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("sf")) install.packages("sf", dependencies = TRUE)
```

## Installation of packages necessary for tvdbn
```{r install tvdbn, include=FALSE}
if(!require("glmnet")) install.packages("glmnet")
if(!require("tvdbn")) install.packages("tvdbn")
if(!require("purrr")) install.packages("purrr")
if(!require("abind")) install.packages("abind")
```

## Loading of packages necessary for bnlearn/dbnR

```{r load dbnR, include=FALSE}
# Basic usage
library(bnlearn)
library(data.table)
library(dbnR)
# Network visualization
library(visNetwork)
library(magrittr)
library(grDevices)
```

## Loading of packages necessary for NCEP/NCAR

```{r load RNCEP, include=FALSE}
library(RNCEP)
library(lubridate) #date and time manipulation
library(tidyverse) #data manipulation and visualization
library(RColorBrewer) #color schemes
library(sf) #to import a spatial object and to work with geom_sf in ggplot2
```

## Loading of packages necessary for tvdbn

```{r load tvdbn, include=FALSE}
library(tvdbn)
library(purrr) 
library(abind)
```

## Dataset loading



We have to define four ranges: One for the months, another for the year, for the latitude and for the longitud. Then, we use the method NCEP.gather to retrieve the data within those ranges of a certain variables (in the example the air temperature at pressure levels of 850 is selected).

```{r load_data}
month_range <- c(1,12)     #period of months
year_range <- c(2020,2020) #period of years

lat_range <- c(30,80)      #latitude range
lon_range <- c(-30,60)     #longitude range
 

data_climate <- NCEP.gather("air",    #name of the variable
                    850, #pressure level 850hPa
                    month_range,year_range,
                    lat_range,lon_range,
                    return.units = TRUE,
                    reanalysis2=TRUE)
```

## Process dataset

First, we define a function that let us reduce the resolution.
```{r resolution_function}
reduce_resolution <- function(data_NCEP,factor) {
  del_lat = 1:length(data_NCEP[,1,1])
  del_lon = 1:length(data_NCEP[1,,1])
  ##
  del_lat = del_lat[del_lat %% factor != 0]
  del_lon = del_lon[del_lon %% factor != 0]

  return(data_NCEP[-del_lat,-del_lon,])
}
```

Now, we reduce the dimensionalty. We aggregate by months and reduce the resolution in order to get approx 50 features
```{r month_aggregate}
data_climate_pr = NCEP.aggregate(data_climate, DAYS = FALSE, HOURS = FALSE, fxn = "mean")
data_climate_pr = reduce_resolution(data_climate_pr, round(sqrt(length(data_climate_pr[,1,1])*length(data_climate_pr[,1,1])/20))  )
```

## Train a web with this data
First, we would have to convert the array into a proper workable dataset. To do so, we define the function transform to dataset.

```{r transform_function}
transform_to_dataset <- function(data_NCEP) {
  names <- c()
  for (i in as.double(dimnames(data_NCEP)[[1]])) {
    for (j in as.double(dimnames(data_NCEP)[[2]])) {
      i_aux = i
      if (i <0) {
        i_aux = paste(-i, "S", sep = "")
      }
      else {
        i_aux = paste(i, "N", sep = "")
      }
      j_aux = j
      if (j <0) {
        j_aux = paste(-j, "W", sep= "")
      }
      else {
        j_aux = paste(j,"E", sep= "")
      }
      name <- paste("lat",i_aux,"_lon",j_aux,"",sep="")
      names <- append(names, name)
    }
  }
  dframe <- data.frame()
  for (k in dimnames(data_NCEP)[[3]]) {
    dframe <- rbind(dframe, as.vector(t(data_NCEP[,,k])))
  }
  colnames(dframe) <- names
  return(dframe)
}
```



```{r transform_data}
climate <- transform_to_dataset(data_NCEP = data_climate_pr)
```


We can undo the transformation as well
```{r undo_transform}
undo = dataset_to_3darray(as.matrix(climate))
```

We learn a tvdbn
```{r learn_tvdbn_climate}
# Normalize using z-score
climate_z = climate
means = colMeans(climate)
sds = sapply(climate, sd, na.rm = TRUE)

for (i in 1:ncol(climate)) {
  climate_z[,i] = (climate[,i]-means[i])/sds[i]
}

climate_z <- as.matrix(climate_z)

tvdbn_climate.fit = tvdbn::learn_tvdbn(climate_z, type = "autoregressive", max_parents = 5, kernel_bandwidth = (length(climate_z[,1])^2)/49*(1))
tvdbn_spatial_climate.fit = tvdbn::learn_tvdbn(climate_z, kernel_bandwidth = (length(climate_z[,1])^2)/49*(1), spatial_penalty = 0.2)
```
## ValidaciÃ³n con motor dataset
```{r import_motor, include=FALSE}
tmp = read.csv("../data/motor_measures_v2_red.csv")
#tmp = tmp[1:6]

# Normalize using z-score
means = colMeans(tmp)
sds = sapply(tmp, sd, na.rm = TRUE)

for (i in 2:ncol(tmp)) {
  tmp[,i] = (tmp[,i]-means[i])/sds[i]
}

# Study the length of the time series that benefits us the more
# We need a trade-off between having a large set of series and having long series
p = c()
m = c()
for (i in seq(0,600,10)) {
  p = c(p, length(table(tmp["profile_id"])[table(tmp["profile_id"]) > i]))
  m = c(m,min(table(tmp["profile_id"])[table(tmp["profile_id"]) > i]))
}
plot(seq(0,600,10), p*m)

# Remove series with under 150 time points
min_length = 150
interest_series = table(tmp["profile_id"])[table(tmp["profile_id"]) > min_length]

max_length = min(interest_series)
#max_length = 50

library(abind)
# Truncate every serie to have only max_length time points and add them in an 3D array
mat_list = list()
for (i in 1:length(interest_series)) {
  serie_i = tmp[tmp[,"profile_id"] == as.integer(names(interest_series))[i],]
  mat_list[[i]] = head(serie_i[,-1],max_length)
}

array_3d = abind(mat_list, along = 3)
#tvdbn_motor = tvdbn::learn_tvdbn(array_3d)

```


```{r cv_motor, include=FALSE}
# First, make the folds
n_folds = 5
randomized_index = sample(1:length(array_3d[1,1,]))
split_index = split(randomized_index, cut(seq_along(randomized_index),n_folds,labels = FALSE))
folds = list()
for (i in 1:n_folds) {
  folds[[i]] = array_3d[,,split_index[[i]]]
}

# Select the interest kernel widths
kernel_widths = c(0.5,50,500,5000,500000) #(0.5,50,500,5000)
# Select the threshold for the global_graph
threshold = 0.95
# Select the number of points of partial inference
p_inference = 5
# Last time point
last_tp = length(array_3d[,1,1])-1

# Train and validate
mse = matrix(0, nrow = length(kernel_widths), ncol = p_inference+1, dimnames = list(as.character(kernel_widths), c("full", as.character(round(last_tp/p_inference*0:(p_inference-1)))) )  )
mse_sd = mse
mse_unfair = mse
mse_unfair_sd = mse

time = c()
hamming = c()
mean_arcs = c()
pers_arcs = c()
pers_arcs_total = c()
time_sd = c()
hamming_sd = c()
mean_arcs_sd = c()
pers_arcs_sd = c()
pers_arcs_total_sd = c()


for (k in kernel_widths) {
  mse_k = matrix(0, ncol = p_inference+1, dimnames = list(NULL, c("full", as.character(round(last_tp/p_inference*0:(p_inference-1)))) )  )
  mse_unfair_k = mse_k
  
  time_k = c()
  hamming_k = c()
  mean_arcs_k = c()
  pers_arcs_k = c()
  pers_arcs_total_k = c()
  
  for (i in 1:n_folds) {
    train_data = abind(folds[-i])
    test_data = folds[[i]]
    t0 = Sys.time()
    network = tvdbn::learn_tvdbn(train_data, kernel_bandwidth = k)
    tf = Sys.time()
    # Store time
    time_k = c(time_k, tf-t0)
    # Store the hamming distance between each consecutive instant
    hamming_k = c(hamming_k, tvdbn::hamming_changes(network))
    # Store the number of arcs per transition network
    mean_arcs_k = c(mean_arcs_k, tvdbn::n_arcs(network))
    # Store the % of persistent arcs of the network (in relation with the average number of arcs per transition network)
    pers_arcs_k = c(pers_arcs_k, length(arcs(global_graph(network, threshold = threshold))[,1])/tvdbn::n_arcs(network)*100)
    # Number of persistent arcs in relations with the set of total arcs
    pers_arcs_total_k = c(pers_arcs_total_k, length(arcs(global_graph(network, threshold = threshold))[,1])/length(arcs(all_arc_graph(network))[,1])*100)
    
    # Mse with test dataset
    for (j in 1:length(test_data[1,1,])) {
      mse_i = c()
      mse_i["full"] = tvdbn::inference(network, test_data[,,j],ini=0,end=length(array_3d[,1,1])-1)[[4]]
      for (sp in 1:p_inference-1) {
        mse_i[as.character(round(last_tp/5*sp))] = tvdbn::inference(network, test_data[,,j],ini=round(last_tp/5*sp),end=round(last_tp/5*(sp+1)))[[4]]
      }
      mse_k = rbind(mse_k,mse_i)
    }
    
    
    # Mse with train dataset
    for (j in 1:length(train_data[1,1,])) {
      mse_unfair_i = c()
      mse_unfair_i["full"] = tvdbn::inference(network, train_data[,,j],ini=0,end=length(array_3d[,1,1])-1)[[4]]
      for (sp in 1:p_inference-1) {
        mse_unfair_i[as.character(round(last_tp/5*sp))] = tvdbn::inference(network, train_data[,,j],ini=round(last_tp/5*sp),end=round(last_tp/5*(sp+1)))[[4]]
      }
      mse_unfair_k = rbind(mse_unfair_k, mse_unfair_i)
    }
    
    
  } # end for i in folds
  
  mse[as.character(k),1] = mean(mse_k[,1])
  for (i in 1:p_inference+1) {
    mse[as.character(k),i] = mean(mse_k[,i])
  }
  mse_sd[as.character(k),1] = sd(mse_k[,1])
  for (i in 1:p_inference+1) {
    mse_sd[as.character(k),i] = sd(mse_k[,i])
  }
  
  mse_unfair[as.character(k),1] = mean(mse_unfair_k[,1])
  for (i in 1:p_inference+1) {
    mse_unfair[as.character(k),i] = mean(mse_unfair_k[,i])
  }
  mse_unfair_sd[as.character(k),1] = sd(mse_unfair_k[,1])
  for (i in 1:p_inference+1) {
    mse_unfair_sd[as.character(k),i] = sd(mse_unfair_k[,i])
  }
  
  time[as.character(k)] = mean(time_k)
  hamming[as.character(k)] = mean(hamming_k)
  mean_arcs[as.character(k)] = mean(mean_arcs_k)
  pers_arcs[as.character(k)] = mean(pers_arcs_k)
  pers_arcs_total[as.character(k)] = mean(pers_arcs_total_k)
  
  time_sd[as.character(k)] = sd(time_k)
  hamming_sd[as.character(k)] = sd(hamming_k)
  mean_arcs_sd[as.character(k)] = sd(mean_arcs_k)
  pers_arcs_sd[as.character(k)] = sd(pers_arcs_k)
  pers_arcs_total_sd[as.character(k)] = sd(pers_arcs_total_k)
  
}

print(mean(mse))
print(mean(mse_unfair))
print(mean(time))

```

```{r store_results}
exp_name = "exp2"
to_export = list("mse"=mse, "mse_sd" = mse_sd, 
                 "time" = time, "time_sd" = time_sd, 
                 "hamming"=  hamming, "hamming_sd"=hamming_sd, 
                 "mean_arcs" = mean_arcs, "mean_arcs_sd" = mean_arcs_sd, 
                 "pers_arcs" = pers_arcs, "pers_arcs_sd" = pers_arcs_sd, 
                 "pers_arcs_total" = pers_arcs_total, "pers_arcs_total_sd" = pers_arcs_total_sd)

file = paste("../experiments/",exp_name,".csv", sep="")
for (i in names(to_export)) {
  write.table(to_export[i], file = file, append = TRUE)
}
```



## Experiments with multiple spatial series
```{r load_data}
years = 1981:2020
lat_range <- c(30,80)      #latitude range
lon_range <- c(-30,60)     #longitude range

data_climate = NULL
for (i in years) {
  year_range <- c(i,i) #period of years
  data_climate_i <- NCEP.gather("air",    #name of the variable
                      850, #pressure level 850hPa
                      c(12,12),year_range-1,
                      lat_range,lon_range,
                      return.units = TRUE,
                      reanalysis2=TRUE)
  data_climate_i <- abind(data_climate_i,NCEP.gather("air",    #name of the variable
                      850, #pressure level 850hPa
                      c(1,12),year_range,
                      lat_range,lon_range,
                      return.units = TRUE,
                      reanalysis2=TRUE), along = 3)
  data_climate_i = NCEP.aggregate(data_climate_i, DAYS = FALSE, HOURS = FALSE, fxn = "mean")
data_climate_i = reduce_resolution(data_climate_i, round(sqrt(length(data_climate_i[,1,1])*length(data_climate_i[,1,1])/30))  )
  data_climate = abind(data_climate, as.matrix(transform_to_dataset(data_climate_i)), along = 3)
}

# Once we have our data, we will apply z-score to normalize its values.
# As we are dealing only with temperatues, we might apply the same normalization every column.
# In this experiments, for the sake of validation, we will apply a different z-score to every column
for (i in 1:ncol(data_climate)) {
  data_climate[,i,] = (data_climate[,i,]-mean(data_climate[,i,]))/sd(data_climate[,i,])
}
```
